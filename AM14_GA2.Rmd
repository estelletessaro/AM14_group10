---
title: "Empirical Finance Group Assignment"
author: "Group 10"
date: "2/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries 

```{r, Load libraries}
library(dplyr)
library(readxl)
library(ggplot2)
library(TTR)
library(tidyverse)
library(lubridate)
```

# Question 1

Pick Microsoft, XOM and GE as the samples.

```{r}
data_q1 <- readxl::read_excel('PS1_Daily.xlsx',sheet=2)
LogReturn <- function(vx) return(diff(log(vx)))
EWMA=function(sigma0,lambda,vs){
  s=rep(0,length(vs))
  s0=sigma0
  s[1]=lambda*s0+(1-lambda)*vs[1]
  for (i in 2:length(vs)){
    s[i]=s[i-1]*lambda+lambda*vs[i]
  }
  return(s)
}

mr <- apply(data_q1[,2:8], 2, LogReturn)
```

- Estimate three volatility time series for each of these three stocks by either using a MA (10 weeks) or an EWMA

```{r}
# Stock 1. Microsoft
Microsoft_logreturn<-mr[,1]
squared_return<-Microsoft_logreturn^2
volatility1<-SMA(squared_return,n=70)
sigma0=sqrt(mean(volatility1[71:6300]^2))
volatility2<-EWMA(0,0.94,squared_return)
# Stock 2. XOM
squared_return<-mr[,2]^2
volatility1<-SMA(squared_return,n=70)
sigma0=sqrt(mean(volatility1[71:6300]^2))
volatility2<-EWMA(0,0.94,squared_return)
# Stock 3. GE
squared_return<-mr[,3]^2
volatility1<-SMA(squared_return,n=70)
sigma0<-sqrt(mean(volatility1[71:6300]^2))
volatility2<-EWMA(0,0.94,squared_return)
```

Now we have six time series (two volatility time series for each of the three stocks), then we need to calculate the daily one day Value-at-Risk (VaR) 95% assuming normality.

```{r}
# Stock 1. Microsoft
VaR1_MA<-mean(mr[,1])-1.65*volatility1
VaR1_EWMA<-mean(mr[,1])-1.65*volatility2
# Stock 2. XOM
VaR2_MA<-mean(mr[,2])-1.65*volatility1
VaR2_EWMA<-mean(mr[,2])-1.65*volatility2
# Stock 3. GE
VaR3_MA<-mean(mr[,3])-1.65*volatility1
VaR3_EWMA<-mean(mr[,3])-1.65*volatility2
```

After counting the negativerealized market returns that are more extreme than the VaR on this given day. If we use MA to predict, there are 2941 violatios in stock 1(MSFT), 2967 violations in stock 2(XOM) and 2970 violations in stock 3(GE); If we use EWMA to predict, there are 1648 violatios in stock 1(MSFT), 2198 violations in stock 2(XOM) and 2006 violations in stock 3(GE);

We can conclude that the volatility estimated by EWMA is more accurate and more in line with the actual changes. The risk estimation of VaR is more radical. The negative returns of most markets are less than the estimated results of var. It shows that VaR underestimates the risk of the market.

# Question 2

```{r, reading f&f data}
#Reading the Fama/French 3 factors at monthly frequency
ff_data <- read.csv("F-F_Research_Data_Factors.csv", skip = 3, nrows = 1146) %>% 
  #Cleaning column names
  janitor::clean_names()
#Renaming date column as it is named x by default
names(ff_data)[names(ff_data) == 'x'] <- 'date'
#Adding 01 date for formating
ff_data <- ff_data %>% 
  mutate(
    date = paste(date, "01", sep="")
  )
#Converting date to date column
ff_data <- ff_data %>%
  mutate(
    date = as.Date(date, "%Y%m%d")
  )
```

```{r, reading op data}
#Reading portfolios formed on Operating Profitability
pf_op_data <- read.csv("Portfolios_Formed_on_OP.csv", skip = 24, nrows = 702) %>% 
  #Cleaning column names
  janitor::clean_names()
#Renaming date column as it is named x by default
names(pf_op_data)[names(pf_op_data) == 'x'] <- 'date'
#Adding 01 date for formating
pf_op_data <- pf_op_data %>% 
  mutate(
    date = paste(date, "01", sep="")
  ) %>% 
  select(date, lo_10, dec_2, dec_3, dec_4, dec_5, dec_6, dec_7, dec_8, dec_9, hi_10)
#Converting date to date column
pf_op_data <- pf_op_data %>%
  mutate(
    date = as.Date(date, "%Y%m%d")
  )
#Joining data on date
data_op <- left_join(pf_op_data, ff_data, by = "date")
```

```{r}
#Getting names of portfolios
pf_names <- colnames(data_op)[2:11]
#Initializing a dataframe with date
data_op_excess = data.frame(data_op$date)
#Looping over all portfolios to calculate excess returns
for (i in 1:length(pf_names)){
  pf_name = pf_names[i]
  #Excess return
  data_op_excess[, paste(pf_name, "_excess_returns")] = data_op[, pf_name] - data_op$rf
}
#Calculating weight of each portfolio
weight = 1 / length(pf_names)
#Multiplying each portfolio's excess return by weight
data_op_excess[, -1] <- data_op_excess[, -1] * weight
#Summing up returns for all portfolios
total_returns <- rowSums(data_op_excess[, -1])
#Combining date and total returns
data_op_excess <- data.frame(data_op$date, total_returns)
#Renaming date column as it is named x by default
names(data_op_excess) <- c('date', 'op_returns')
```

```{r, reading inv data}
#Reading portfolios formed on Investment
pf_inv_data <- read.csv("Portfolios_Formed_on_INV.csv", skip = 17, nrows = 702) %>% 
  #Cleaning column names
  janitor::clean_names()
#Renaming date column as it is named x by default
names(pf_inv_data)[names(pf_inv_data) == 'x'] <- 'date'
#Adding 01 date for formating
pf_inv_data <- pf_inv_data %>% 
  mutate(
    date = paste(date, "01", sep="")
  ) %>% 
  select(date, lo_10, dec_2, dec_3, dec_4, dec_5, dec_6, dec_7, dec_8, dec_9, hi_10)
#Converting date to date column
pf_inv_data <- pf_inv_data %>%
  mutate(
    date = as.Date(date, "%Y%m%d")
  )
#Joining data on date
data_inv <- left_join(pf_inv_data, ff_data, by = "date")
```

```{r}
#Getting names of portfolios
pf_names <- colnames(data_inv)[2:11]
#Initializing a dataframe with date
data_inv_excess = data.frame(data_inv$date)
#Looping over all portfolios to calculate excess returns
for (i in 1:length(pf_names)){
  pf_name = pf_names[i]
  #Excess return
  data_inv_excess[, paste(pf_name, "_excess_returns")] = data_inv[, pf_name] - data_inv$rf
}
#Calculating weight of each portfolio
weight = 1 / length(pf_names)
#Multiplying each portfolio's excess return by weight
data_inv_excess[, -1] <- data_inv_excess[, -1] * weight
#Summing up returns for all portfolios
total_returns <- rowSums(data_inv_excess[, -1])
#Combining date and total returns
data_inv_excess <- data.frame(data_inv$date, total_returns)
#Renaming date column as it is named x by default
names(data_inv_excess) <- c('date', 'inv_returns')
```

```{r, reading div data}
#Reading portfolios formed on Dividend Yield
pf_div_data <- read.csv("Portfolios_Formed_on_D-P.csv", skip = 19, nrows = 1134) %>% 
  #Cleaning column names
  janitor::clean_names()
#Renaming date column as it is named x by default
names(pf_div_data)[names(pf_div_data) == 'x'] <- 'date'
#Adding 01 date for formating
pf_div_data <- pf_div_data %>% 
  mutate(
    date = paste(date, "01", sep="")
  ) %>% 
  select(date, lo_10, dec_2, dec_3, dec_4, dec_5, dec_6, dec_7, dec_8, dec_9, hi_10)
#Converting date to date column
pf_div_data <- pf_div_data %>%
  mutate(
    date = as.Date(date, "%Y%m%d")
  )
#Joining data on date
data_div <- left_join(pf_div_data, ff_data, by = "date")
```

```{r}
#Getting names of portfolios
pf_names <- colnames(data_div)[2:11]
#Initializing a dataframe with date
data_div_excess = data.frame(data_div$date)
#Looping over all portfolios to calculate excess returns
for (i in 1:length(pf_names)){
  pf_name = pf_names[i]
  #Excess return
  data_div_excess[, paste(pf_name, "_excess_returns")] = data_div[, pf_name] - data_div$rf
}
#Calculating weight of each portfolio
weight = 1 / length(pf_names)
#Multiplying each portfolio's excess return by weight
data_div_excess[, -1] <- data_div_excess[, -1] * weight
#Summing up returns for all portfolios
total_returns <- rowSums(data_div_excess[, -1])
#Combining date and total returns
data_div_excess <- data.frame(data_div$date, total_returns)
#Renaming date column as it is named x by default
names(data_div_excess) <- c('date', 'div_returns')
```

```{r, reading momentum data}
#Reading portfolios formed on Momentum
pf_mom_data <- read.csv("10_Portfolios_Prior_12_2.csv", skip = 10, nrows = 1140) %>% 
  #Cleaning column names
  janitor::clean_names()
#Renaming date column as it is named x by default
names(pf_mom_data)[names(pf_mom_data) == 'x'] <- 'date'
#Adding 01 date for formating
pf_mom_data <- pf_mom_data %>% 
  mutate(
    date = paste(date, "01", sep="")
  )
#Converting date to date column
pf_mom_data <- pf_mom_data %>%
  mutate(
    date = as.Date(date, "%Y%m%d")
  )
#Joining data on date
data_mom <- left_join(pf_mom_data, ff_data, by = "date")
```


```{r}
#Getting names of portfolios
pf_names <- colnames(data_mom)[2:11]
#Initializing a dataframe with date
data_mom_excess = data.frame(data_mom$date)
#Looping over all portfolios to calculate excess returns
for (i in 1:length(pf_names)){
  pf_name = pf_names[i]
  #Excess return
  data_mom_excess[, paste(pf_name, "_excess_returns")] = data_mom[, pf_name] - data_mom$rf
}
#Calculating weight of each portfolio
weight = 1 / length(pf_names)
#Multiplying each portfolio's excess return by weight
data_mom_excess[, -1] <- data_mom_excess[, -1] * weight
#Summing up returns for all portfolios
total_returns <- rowSums(data_mom_excess[, -1])
#Combining date and total returns
data_mom_excess <- data.frame(data_mom$date, total_returns)
#Renaming date column as it is named x by default
names(data_mom_excess) <- c('date', 'mom_returns')
```

```{r, reading 49 industry data}
#Reading 49 indsustry portfolios
pf_49i_data <- read.csv("49_Industry_Portfolios.csv", skip = 11, nrows = 1146) %>% 
  #Cleaning column names
  janitor::clean_names()
#Renaming date column as it is named x by default
names(pf_49i_data)[names(pf_49i_data) == 'x'] <- 'date'
#Adding 01 date for formating
pf_49i_data <- pf_49i_data %>% 
  mutate(
    date = paste(date, "01", sep="")
  )
#Converting date to date column
pf_49i_data <- pf_49i_data %>%
  mutate(
    date = as.Date(date, "%Y%m%d")
  )
#Joining data on date
data_49i <- left_join(pf_49i_data, ff_data, by = "date")
```

```{r}
#Getting names of portfolios
pf_names <- colnames(data_49i)[2:11]
#Initializing a dataframe with date
data_49i_excess = data.frame(data_49i$date)
#Looping over all portfolios to calculate excess returns
for (i in 1:length(pf_names)){
  pf_name = pf_names[i]
  #Excess return
  data_49i_excess[, paste(pf_name, "_excess_returns")] = data_49i[, pf_name] - data_49i$rf
}
#Calculating weight of each portfolio
weight = 1 / length(pf_names)
#Multiplying each portfolio's excess return by weight
data_49i_excess[, -1] <- data_49i_excess[, -1] * weight
#Summing up returns for all portfolios
total_returns <- rowSums(data_49i_excess[, -1])
#Combining date and total returns
data_49i_excess <- data.frame(data_49i$date, total_returns)
#Renaming date column as it is named x by default
names(data_49i_excess) <- c('date', '49i_returns')
```


# Question 3

```{r, combining excess returns of all portfolios}
excess_return_data <- inner_join(data_op_excess, data_inv_excess, by = "date")
excess_return_data <- inner_join(excess_return_data, data_div_excess, by = "date")
excess_return_data <- inner_join(excess_return_data, data_mom_excess, by = "date")
excess_return_data <- inner_join(excess_return_data, data_49i_excess, by = "date")

excess_return_data_v1 <- excess_return_data[, -c(1)]
```

```{r}
pca = prcomp(excess_return_data_v1, retx = TRUE)
summary(pca)
plot(pca)
```

To explain 95% of the return variation, only 1 component is needed. The output results and images show that PC1 contributes 96.1% to the cumulative variance, so that only 1 component can be used to explain the results.

# Question 4

```{r Regression FF, warning = FALSE}
reg_joined <- merge(pf_div_data, pf_op_data, by = "date") %>%
  merge(pf_inv_data, by = "date") %>%
  merge(pf_mom_data, by = "date") %>%
  merge(pf_49i_data, by = "date") %>% 
  merge(ff_data, by = "date")

# Obtain Portfolio Names
portfolios <- (as.data.frame(t(colnames(reg_joined)[2:90])))

# Create a blank data frame to store regression results for different portfolios
regression_results <- data.frame(matrix(ncol = 1, nrow = 89))

# Assign column names and row names for this data frame
colnames(regression_results) <- c("Adjusted R Squared") 
rownames(regression_results) <- portfolios # Rows are different portfolios

# Assign 'count' for loop results
count <- c(1:89)
  
# Initiate the for loop
for (i in count) {
  # Create a dummy data set for the regression
  reg_dummy <- reg_joined %>%
    # Mutate to create the y variable for the regression - loop dependent
    mutate(y = .[[i+1]] - .[[94]])
  # Run the regression within the loop
  reg_fit_dummy = lm(y~mkt_rf + smb + hml, data = reg_dummy)
  # print(summary(reg_fit_dummy)[9])
  # Print out the R^2 for the regression - add to data frame of results
  regression_results[i,1] <- summary(reg_fit_dummy)[9]
}


# Finally, print out results of the adjusted R^2 for all portfolios...

# Mean of the adjusted R^2
print(mean(as.numeric(unlist(regression_results))))

# Median of the adjusted R^2
print(median(as.numeric(unlist(regression_results))))

# Standard Deviation of the adjusted R^2
print(sd(as.numeric(unlist(regression_results))))

```



# Question 5

```{r PCA regression, warning = FALSE }
reg_joined_2 <- merge(pf_div_data, pf_op_data, by = "date") %>%
  merge(pf_inv_data, by = "date") %>%
  merge(pf_mom_data, by = "date") %>%
  merge(pf_49i_data, by = "date") %>% 
  merge(ff_data, by = "date")
# Obtain Portfolio Names
portfolios_2 <- (as.data.frame(t(colnames(reg_joined)[2:90])))
# Create a blank data frame to store regression results for different portfolios
regression_results_2 <- data.frame(matrix(ncol = 1, nrow = 89))
# Assign column names and row names for this data frame
colnames(regression_results_2) <- c("Adjusted R Squared") 
rownames(regression_results_2) <- portfolios # Rows are different portfolios
# Assign 'count' for loop results
count <- c(1:89)
# Initiate the for loop
for (i in count) {
  # Create a dummy data set for the regression
  reg_dummy_2 <- reg_joined_2 %>%
    # Mutate to create the y variable for the regression - loop dependent
    mutate(y = .[[i+1]] - .[[94]])
  # Run the regression within the loop
  reg_fit_dummy_2 = lm(reg_dummy_2$y ~ pca$x[,1] + pca$x[,2] + pca$x[,3])
  # print(summary(reg_fit_dummy)[9])
  # Print out the R^2 for the regression - add to data frame of results
  regression_results_2[i,1] <- summary(reg_fit_dummy_2)[9]
}


# Finally, print out results of the adjusted R^2 for all portfolios...
# Mean of the adjusted R^2
print(mean(as.numeric(unlist(regression_results_2))))
# Median of the adjusted R^2
print(median(as.numeric(unlist(regression_results_2))))
# Standard Deviation of the adjusted R^2
print(sd(as.numeric(unlist(regression_results_2))))
```

As we can see the results of the second regression (involving the PCA terms) 
exhibit a slight improvement on the first regression (which was performed using
the FAMA-FRENCH 3 factors). Notably, there is a slight increase in the mean R^2 
for the different portfolios - this suggests that the regression models (with the 
PCA terms) are better explaining the variability in the returns of the portfolios
(when compared to the regression models involving the F-F terms). There is also 
less variation in the R^2 amongst different portfolios for the second regression 
type. 


# Question 6

What are the problems with factor models based on principal components?

The biggest limitation of principal components is interpretability. Principle Component are linear combinations of the returns for each date. Conversely,Fama-French 3 factors are highly interpretable, as they are specific characteristics of stocks - size of firm, book-to-market value, and excess return on the market. Therefore, the PC components lack in supporting financial intuition. Furthermore, PC components suffer from overfitting, as they are constructed based on a specific dataset, which is not representative of future stock movements and can lead to invalid results.

# Question 7

Go to CANVAS and download the data file PS4 Daily.xlsx. This file contains daily yield curve data
for the United States between July 2 1981 and January 31 2020. In particular, you are given spot rates
for 1-year, 2-years, ..., 20-years.

```{r, warning = FALSE}
data_q7 <- readxl::read_excel('PS4_Daily.xlsx') %>% 
  janitor::clean_names() %>%
  mutate_if(is.character, as.numeric) %>%
  drop_na() %>%
  mutate(date = ymd(date)) %>%
  select(everything(), -date)

skimr::skim(data_q7) 
```


# Question 8

Use principal component analysis to examine the data. How many principal components are needed to
explain the majority of the variation in the yields (hint: run prcomp on the yields and not on changes
in yields)? Extract the first three components and plot them in a time series plot (again, you can
extract them as discussed above).

```{r}
dims <- dim(data_q7)
dims #dimensionality is 9626 by 20

pca <- prcomp(data_q7) 

str(pca)
summary(pca) 
plot(pca)
```

The first component already explains 98.4% of the data and the first two explain 99.9% of the data. The variation in yields is very well explained by just the first principal component.

```{r}
plot(pca$x[,1],main="PC1",xlab="day",ylab="")
plot(pca$x[,2],main="PC2",xlab="day",ylab="")
plot(pca$x[,3],main="PC3",xlab="day",ylab="")
```

```{r}
df <- data.frame(x1=pca$x[,1],
                x2=data_q7$sveny03,
                x3=pca$x[,2],
                x4=data_q7$sveny10-data_q7$sveny01)

cor(df)
```

There is no obvious correlationship among the first component and the second component and the difference between the 10-year and the 1-year yield. The first component and the 3-year yield are significantly negatively correlated.